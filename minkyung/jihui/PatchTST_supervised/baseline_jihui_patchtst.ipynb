{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7444f30-0883-4a87-8e00-c5e5591a2a26",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b85832b1-9f31-41e4-b934-cc260e5b3b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihui/miniconda3/envs/dacon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0259cfd-b37c-4716-aacf-c91d77e41480",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5caa6cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>5.827</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.462</td>\n",
       "      <td>4.203</td>\n",
       "      <td>1.340</td>\n",
       "      <td>30.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>5.693</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.142</td>\n",
       "      <td>1.371</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 02:00:00</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.741</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.355</td>\n",
       "      <td>3.777</td>\n",
       "      <td>1.218</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 03:00:00</td>\n",
       "      <td>5.090</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.391</td>\n",
       "      <td>3.807</td>\n",
       "      <td>1.279</td>\n",
       "      <td>25.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 04:00:00</td>\n",
       "      <td>5.358</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.462</td>\n",
       "      <td>3.868</td>\n",
       "      <td>1.279</td>\n",
       "      <td>21.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17415</th>\n",
       "      <td>2018-06-26 15:00:00</td>\n",
       "      <td>-1.674</td>\n",
       "      <td>3.550</td>\n",
       "      <td>-5.615</td>\n",
       "      <td>2.132</td>\n",
       "      <td>3.472</td>\n",
       "      <td>1.523</td>\n",
       "      <td>10.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17416</th>\n",
       "      <td>2018-06-26 16:00:00</td>\n",
       "      <td>-5.492</td>\n",
       "      <td>4.287</td>\n",
       "      <td>-9.132</td>\n",
       "      <td>2.274</td>\n",
       "      <td>3.533</td>\n",
       "      <td>1.675</td>\n",
       "      <td>11.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17417</th>\n",
       "      <td>2018-06-26 17:00:00</td>\n",
       "      <td>2.813</td>\n",
       "      <td>3.818</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>2.097</td>\n",
       "      <td>3.716</td>\n",
       "      <td>1.523</td>\n",
       "      <td>10.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17418</th>\n",
       "      <td>2018-06-26 18:00:00</td>\n",
       "      <td>9.243</td>\n",
       "      <td>3.818</td>\n",
       "      <td>5.472</td>\n",
       "      <td>2.097</td>\n",
       "      <td>3.655</td>\n",
       "      <td>1.432</td>\n",
       "      <td>9.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17419</th>\n",
       "      <td>2018-06-26 19:00:00</td>\n",
       "      <td>10.114</td>\n",
       "      <td>3.550</td>\n",
       "      <td>6.183</td>\n",
       "      <td>1.564</td>\n",
       "      <td>3.716</td>\n",
       "      <td>1.462</td>\n",
       "      <td>9.567000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17420 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date    HUFL   HULL   MUFL   MULL   LUFL   LULL  \\\n",
       "0      2016-07-01 00:00:00   5.827  2.009  1.599  0.462  4.203  1.340   \n",
       "1      2016-07-01 01:00:00   5.693  2.076  1.492  0.426  4.142  1.371   \n",
       "2      2016-07-01 02:00:00   5.157  1.741  1.279  0.355  3.777  1.218   \n",
       "3      2016-07-01 03:00:00   5.090  1.942  1.279  0.391  3.807  1.279   \n",
       "4      2016-07-01 04:00:00   5.358  1.942  1.492  0.462  3.868  1.279   \n",
       "...                    ...     ...    ...    ...    ...    ...    ...   \n",
       "17415  2018-06-26 15:00:00  -1.674  3.550 -5.615  2.132  3.472  1.523   \n",
       "17416  2018-06-26 16:00:00  -5.492  4.287 -9.132  2.274  3.533  1.675   \n",
       "17417  2018-06-26 17:00:00   2.813  3.818 -0.817  2.097  3.716  1.523   \n",
       "17418  2018-06-26 18:00:00   9.243  3.818  5.472  2.097  3.655  1.432   \n",
       "17419  2018-06-26 19:00:00  10.114  3.550  6.183  1.564  3.716  1.462   \n",
       "\n",
       "              OT  \n",
       "0      30.531000  \n",
       "1      27.787001  \n",
       "2      27.787001  \n",
       "3      25.044001  \n",
       "4      21.948000  \n",
       "...          ...  \n",
       "17415  10.904000  \n",
       "17416  11.044000  \n",
       "17417  10.271000  \n",
       "17418   9.778000  \n",
       "17419   9.567000  \n",
       "\n",
       "[17420 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./dataset/ETTh1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2d47b-e9da-47e5-9155-cce997e63481",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f50013-6513-44fd-8e48-06dd12ec3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
    "    'PREDICT_SIZE':21, # 21일치 예측\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':4096,\n",
    "    'SEED':41,\n",
    "    'LAMBDA':0.8,\n",
    "    'LR_LAMBDA':0.70\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44cdbe67-eda2-42ef-bc35-0a2bfd99f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68c38e",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c49504",
   "metadata": {},
   "source": [
    "- Encode Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b89389",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./train.csv').drop(columns=['ID', '제품'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13076510",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_kuka = train_data.iloc[:,-CFG['PREDICT_SIZE']:].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0f300",
   "metadata": {},
   "source": [
    "- Product Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0540e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_features = pd.read_csv('./features.csv').drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'ID', '제품', '대분류', '중분류', '소분류', '브랜드'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a784fc4",
   "metadata": {},
   "source": [
    "- Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9a4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_features = pd.read_csv('./total_dates_scaling.csv').drop(columns=['Unnamed: 0', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9342e",
   "metadata": {},
   "source": [
    "- Product & Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "318c5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (15890, 459)\n",
    "# price = pd.read_csv('./price.csv').iloc[:,7:]\n",
    "# sale_event = pd.read_csv('./sale_event.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e80a6",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767b99c",
   "metadata": {},
   "source": [
    "- Encode Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa7ba7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자형 변수들의 min-max scaling을 수행하는 코드입니다.\n",
    "numeric_cols = train_data.columns[4:]\n",
    "# 각 column의 min 및 max 계산\n",
    "min_values = train_data[numeric_cols].min(axis=1)\n",
    "max_values = train_data[numeric_cols].max(axis=1)\n",
    "# 각 행의 범위(max-min)를 계산하고, 범위가 0인 경우 1로 대체\n",
    "ranges = max_values - min_values\n",
    "ranges[ranges == 0] = 1\n",
    "# min-max scaling 수행\n",
    "train_data[numeric_cols] = (train_data[numeric_cols].subtract(min_values, axis=0)).div(ranges, axis=0)\n",
    "# max와 min 값을 dictionary 형태로 저장\n",
    "scale_min_dict = min_values.to_dict()\n",
    "scale_max_dict = max_values.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c83c51b-f979-4930-9372-f03bdb33abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "categorical_nums = [5, 11, 53, 3170]\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])\n",
    "    train_data[col] = train_data[col].div(categorical_nums[i]-1) # minmax scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5bb5b",
   "metadata": {},
   "source": [
    "- Product Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d793af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (15890, 1)로 만들기\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler1.fit(product_features[['판매량평균']])\n",
    "sales_mean_scaled = scaler1.transform(product_features[['판매량평균']])\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.fit(product_features[['제품수']])\n",
    "prod_num_scaled = scaler2.transform(product_features[['제품수']])\n",
    "\n",
    "nprice_mid = product_features[['NormalizedPrice_중분류']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356aac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = np.column_stack((sales_mean_scaled, prod_num_scaled, nprice_mid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ce850",
   "metadata": {},
   "source": [
    "- Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ddd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (459, 1)\n",
    "# weekend_salary = (time_series_features[['Weekend']].to_numpy() + 0.5*time_series_features[['Salary']].to_numpy() + 0.5*np.where(time_series_features[['DayofWeek']]==4/6, 1, 0)).squeeze()\n",
    "sale_info = time_series_features['SaleInfo'].to_numpy()\n",
    "holiday = time_series_features['Holiday'].to_numpy()\n",
    "salary = time_series_features['Salary'].to_numpy()\n",
    "month = time_series_features['Month'].to_numpy()\n",
    "dayofweek = time_series_features['DayofWeek'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "842b186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = np.stack((sale_info, salary, month, dayofweek)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e091c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 459)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "debff992-a1f4-4ade-b2e9-45b234e44412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 학습에 활용할 기간\n",
    "    predict_size : 추론할 기간\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    window_size = train_size + predict_size\n",
    "    \n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, 1 + product.shape[1] + time_series.shape[0] + 1))\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size, 1 + product.shape[1] + time_series.shape[0] + 1))\n",
    "    # target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, 0]) # 대분류 소분류\n",
    "        product_data = product[i,:]\n",
    "        # time_series\n",
    "        sales_data = np.array(data.iloc[i, 4:])\n",
    "        # price_data = np.array(price.iloc[i, :])\n",
    "        # sale_event_data = np.array(sale_event.iloc[i, :])\n",
    "        \n",
    "        for j in range(len(sales_data) - window_size + 1):\n",
    "            time_series_window = time_series[:, j : j + window_size]\n",
    "            # price_window = price_data[j : j + window_size]\n",
    "            # sale_event_window = sale_event_data[j : j + window_size]\n",
    "            window = sales_data[j : j + window_size]\n",
    "            \n",
    "            temp_data = np.concatenate((np.tile(encode_info, (train_size, 1)), np.tile(product_data, (train_size, 1)), time_series_window[:,:train_size].T,window[:train_size].reshape((-1,1))), axis=1)\n",
    "            temp_target = np.concatenate((np.tile(encode_info, (predict_size, 1)), np.tile(product_data, (predict_size, 1)), time_series_window[:,train_size:].T, window[train_size:].reshape((-1,1))), axis=1)\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = temp_target\n",
    "    \n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf39b0f-64f4-4126-9a3d-da5de9f624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    '''\n",
    "    평가 데이터(Test Dataset)를 추론하기 위한 Input 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 추론을 위해 필요한 일별 판매량 기간 (= 학습에 활용할 기간)\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    \n",
    "    input_data = np.empty((num_rows, train_size, 1 + product.shape[1] + time_series.shape[0] + 1))\n",
    "    \n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, 0])\n",
    "        product_data = product[i,:]\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "        # price_data = np.array(price.iloc[i, -train_size:])\n",
    "        # sale_event_data = np.array(sale_event.iloc[i, -train_size:])\n",
    "        \n",
    "        time_series_window = time_series[:, -train_size:]\n",
    "        # price_window = price_data[-train_size:]\n",
    "        # sale_event_window = sale_event_data[-train_size:]\n",
    "        window = sales_data[-train_size : ]\n",
    "\n",
    "        temp_data = np.concatenate((np.tile(encode_info, (train_size, 1)), np.tile(product_data, (train_size, 1)), time_series_window[:,:train_size].T, window[:train_size].reshape((-1,1))),axis=1)\n",
    "        input_data[i] = temp_data\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b2f3d76-fcf4-4866-a578-6bb76783bbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15890/15890 [02:15<00:00, 117.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_input, train_target = make_train_data(train_data) # train val total\n",
    "train_input, train_target = make_train_data(train_data.iloc[:,:-CFG['PREDICT_SIZE']]) # train val\n",
    "# test_input_kuka = make_predict_data(train_data.iloc[:,:-CFG['PREDICT_SIZE']]) # test(kuka pre-submission)\n",
    "# test_input = make_predict_data(train_data) # test(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f3d76-fcf4-4866-a578-6bb76783bbed",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ec0a970-4d99-486d-b9b5-210f3cdca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3614347b-da14-466f-9d04-b81e5448a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_input, train_target)\n",
    "\n",
    "data_len = len(dataset)\n",
    "train_size = int(data_len*0.8)\n",
    "val_size = int(data_len*0.2)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f0b66-817d-49ff-9163-a975fb0f239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_iter = enumerate(train_loader)\n",
    "# i, batch_train = train_loader_iter.__next__()\n",
    "\n",
    "# X, Y = batch_train\n",
    "# print(X.shape)\n",
    "# print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a1be5",
   "metadata": {},
   "source": [
    "## Prepare the forecaster 🏋️‍♂️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f0b66-817d-49ff-9163-a975fb0f239d",
   "metadata": {},
   "source": [
    "### 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f79f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_size=13, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size//2, output_size)\n",
    "        )\n",
    "            \n",
    "        self.actv = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, TRAIN_WINDOW_SIZE, 13)\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # Only use the last output sequence\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(last_output))\n",
    "        \n",
    "        return output.squeeze(1)\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state and cell state\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f79f7d",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSFALoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PSFALoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        share_denominator = torch.sum(targets, axis=0)\n",
    "        share_denominator[share_denominator==float(0)] = 1 # 나눠지게 만들자\n",
    "        share = targets/share_denominator\n",
    "        error_demoninator = torch.max(inputs, targets)\n",
    "        error_demoninator[error_demoninator==float(0)] = 1 # 나눠지게 만들자\n",
    "        error = torch.abs(inputs-targets)/error_demoninator\n",
    "        metric = error * share\n",
    "        loss = torch.mean(torch.sum(metric, axis=1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs_bigcat={}\n",
    "for bigcat in train_data['대분류'].unique():\n",
    "    indexs_bigcat[bigcat] = list(train_data.loc[train_data['대분류']==bigcat].index)\n",
    "\n",
    "indexs_bigcat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88930e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSFA(pred, target): \n",
    "    PSFA = 1\n",
    "    for cat in range(5):\n",
    "        ids = indexs_bigcat[cat]\n",
    "        for day in range(21):\n",
    "            total_sell = np.sum(target[ids, day]) # day별 총 판매량\n",
    "            pred_values = pred[ids, day] # day별 예측 판매량\n",
    "            target_values = target[ids, day] # day별 실제 판매량\n",
    "            \n",
    "            # 실제 판매와 예측 판매가 같은 경우 오차가 없는 것으로 간주 \n",
    "            denominator = np.maximum(target_values, pred_values)\n",
    "            diffs = np.where(denominator!=0, np.abs(target_values - pred_values) / denominator, 0)\n",
    "            \n",
    "            if total_sell != 0:\n",
    "                sell_weights = target_values / total_sell  # Item별 day 총 판매량 내 비중\n",
    "            else:\n",
    "                sell_weights = np.ones_like(target_values) / len(ids)  # 1 / len(ids)로 대체\n",
    "                \n",
    "            if not np.isnan(diffs).any():  # diffs에 NaN이 없는 경우에만 PSFA 값 업데이트\n",
    "                PSFA -= np.sum(diffs * sell_weights) / (21 * 5)\n",
    "            \n",
    "            \n",
    "    return PSFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73d757-32d5-4868-afbb-1b9f2ea13826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    psfa = PSFALoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_mae = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(X)\n",
    "            \n",
    "            loss = (1-CFG['LAMBDA'])*criterion(output, Y) + CFG['LAMBDA']*psfa(output, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        scheduler.step() # added\n",
    "        \n",
    "        val_loss = validation(model, val_loader, criterion, psfa, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "        \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83fa73-30d5-489c-852b-d655f76a200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, psfa, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            output = model(X)\n",
    "            \n",
    "            loss = (1-CFG['LAMBDA'])*criterion(output, Y) + CFG['LAMBDA']*psfa(output, Y)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83fa73-30d5-489c-852b-d655f76a200c",
   "metadata": {},
   "source": [
    "## Run !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b20af7-f5b1-4a7a-8eb9-7dde5bbf3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel()\n",
    "print(torch.cuda.device_count())\n",
    "model = nn.DataParallel(model) # parallel mode\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: CFG[\"LR_LAMBDA\"] ** epoch)\n",
    "infer_model = train(model, optimizer, scheduler, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609c9ea",
   "metadata": {},
   "source": [
    "- 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(infer_model.state_dict, './baseline_submit_0818_ver1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b20af7-f5b1-4a7a-8eb9-7dde5bbf3d04",
   "metadata": {},
   "source": [
    "## 모델 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48def841",
   "metadata": {},
   "source": [
    "- 저장된 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 추가\n",
    "infer_model = BaseModel().to(device)\n",
    "print(torch.cuda.device_count())\n",
    "infer_model = nn.DataParallel(infer_model) # parallel mode\n",
    "\n",
    "infer_model.load_state_dict(torch.load('./baseline_submit_0818_ver1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d7ca0-899e-4515-a43e-890549f8f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용\n",
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "# 성능확인용(추가)\n",
    "test_dataset_kuka = CustomDataset(test_input_kuka, None)\n",
    "test_loader_kuka = DataLoader(test_dataset_kuka, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79aff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader_iter = enumerate(test_loader)\n",
    "# i, batch_test = test_loader_iter.__next__()\n",
    "# print(i, batch_test)\n",
    "# X = batch_test\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f30d4-2b19-479f-89b7-bf5bb2adc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "            \n",
    "            output = model(X)\n",
    "            \n",
    "            # 모델 출력인 output을 CPU로 이동하고 numpy 배열로 변환\n",
    "            output = output.cpu().numpy()\n",
    "            \n",
    "            predictions.extend(output)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f3db2",
   "metadata": {},
   "source": [
    "### 성능확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c606d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kuka = inference(infer_model, test_loader_kuka, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e542cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs_bigcat={}\n",
    "for bigcat in train_data['대분류'].unique():\n",
    "    indexs_bigcat[bigcat] = list(train_data.loc[train_data['대분류']==bigcat].index)\n",
    "\n",
    "indexs_bigcat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b47420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대분류 scaling 때문에 추가\n",
    "d = {0.25:1, 0.5:2, 0.0:0, 1.0:4, 0.75:3}\n",
    "indexs_bigcat = dict((d[key], value) for (key, value) in indexs_bigcat.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62158f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 결과를 inverse scaling\n",
    "for idx in range(len(pred_kuka)):\n",
    "    pred_kuka[idx, :] = pred_kuka[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "    \n",
    "# 결과 후처리\n",
    "pred_kuka = np.round(pred_kuka, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e28a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PSFA(pred_kuka, output_kuka))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a1201",
   "metadata": {},
   "source": [
    "### 제출용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76e053-6fd2-44a7-8631-d903e7ffa292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517978aa-445a-4ece-9217-432682f71230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 결과를 inverse scaling\n",
    "for idx in range(len(pred)):\n",
    "    pred[idx, :] = pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "    \n",
    "# 결과 후처리\n",
    "pred = np.round(pred, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b50eb-d2d8-4c2d-a5e7-9607220fd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b50eb-d2d8-4c2d-a5e7-9607220fd794",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c84bb-5dbe-4fb3-aff0-7e229ae29a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db62d9c-b3ad-440a-8cc7-4897b2e4860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.iloc[:,1:] = pred\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142f749-f20f-4797-b586-581e5c778297",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./baseline_submit_0818_ver1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
